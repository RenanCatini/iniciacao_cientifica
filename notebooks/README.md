# Anota√ß√µes sobre Modelos de Machine Learn


# Anota√ß√µes - √Årvore de Decis√£o üå≥

---

### Ideia Principal
* √â tipo um jogo de "Adivinhe Quem?".
* Faz perguntas (sim/n√£o) pra ir eliminando as op√ß√µes at√© sobrar uma s√≥.

### O que √©?
* Modelo de ML em formato de fluxograma / √°rvore.
* Toma decis√µes com base numa sequ√™ncia de perguntas sobre os dados.
* Cada pergunta quebra os dados em grupos menores.

### Como funciona? (A parte importante)

**A grande d√∫vida:** Como a √°rvore escolhe a "melhor" pergunta pra fazer em cada etapa?

**Resposta: PUREZA de um grupo.**

* **Grupo Puro:** Todo mundo no grupo tem a mesma classifica√ß√£o/resposta.
    * Ex: Caixa s√≥ com ma√ß√£s.
    * √â o nosso objetivo. Chegou num grupo puro = FIM. Vira uma **"folha"** da √°rvore.

* **Grupo Impuro (Misto):** As respostas est√£o misturadas.
    * Ex: Caixa com ma√ß√£s e laranjas.
    * PROBLEMA! Precisa fazer mais perguntas pra tentar separar.

**OBJETIVO DO ALGORITMO:** Fazer a pergunta que **mais aumenta a pureza** (ou mais diminui a impureza) dos grupos que v√£o ser criados.

* *Lembrete: Como ele mede a impureza? Usa umas f√≥rmulas matem√°ticas tipo **√çndice Gini** ou **Entropia**, que basicamente d√£o notas para o grupo, o qu√£o impuro ou puro ele est√°.*

### Vantagens e Desvantagens

**üëç Vantagens:**
* F√°cil de entender e visualizar. D√° pra desenhar.
* N√£o precisa de mto pr√©-processamento de dados.

**üëé Desvantagens:**
* **OVERFITTING!!! (MUITO CUIDADO AQUI)**
* O que √©? A √°rvore fica complexa demais e "decora" os dados de treino.
* Ela n√£o aprende a regra geral, s√≥ os exemplos espec√≠ficos.
* **Analogia:** O aluno que decora as respostas da prova. Se mudar a pergunta, ele erra.

### M√£o na Massa: O `DecisionTreeClassifier` do `sklearn` üêç

Na pr√°tica, a gente n√£o implementa isso do zero. Usamos a biblioteca `scikit-learn`.

```python
from sklearn.tree import DecisionTreeClassifier

# Criando o modelo com os par√¢metros padr√£o
modelo_arvore = DecisionTreeClassifier()

# Depois √© s√≥ treinar com os dados
# modelo_arvore.fit(X_treino, y_treino)
```

**Hiperpar√¢metros importantes na hora de criar o modelo:**

```python
    DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)
```


* `criterion='gini'` (ou `'entropy')`
    * √â aqui que a gente escolhe a "r√©gua" pra medir a impureza. Lembra das 'notas'? √â isso.
    * `'gini'` √© o padr√£o e geralmente **mais r√°pido**, funciona calculando a probabilidade de errar (se tiver uma caixa com apenas 1 tipo, o gini=0, se tiver 2 tipos 50/50, o gini ser√° de 0.5). Vai de 0 at√© 0.5 (para 2 classes).
        * Calcula a partir da multiplica√ß√£o simples de probabilidades.
        * $$I_G(p) = 1 - \sum_{i=1}^{k} (p_i)^2$$
        * Onde:
            * **$k$**: √â o n√∫mero total de classes.
            * **$p_i$**: √â a propor√ß√£o (probabilidade) da classe *i* no grupo.
    * `'entropy'` √© geralmente um pouco **mais lento**, funciona calculando o n√≠vel de 'surpresa' ou 'desordem' (se tiver uma caixa com apenas 1 tipo, a surpresa √© 0, se tiver 2 tipos 50/50, a surpresa/incerteza √© m√°xima = 1). Vai de 0 at√© 1.0 (para 2 classes).
        * Calcula usando logaritmos, que vem da Teoria da Informa√ß√£o.
        * $$H(p) = - \sum_{i=1}^{k} p_i \log_2(p_i)$$
        * Onde:
            * **$k$**: √â o n√∫mero total de classes.
            * **$p_i$**: √â a propor√ß√£o (probabilidade) da classe *i* no grupo.

* `max_depth=None` (ou um n√∫mero, ex: `3`)
    * **Profundidade m√°xima** da √°rvore. Quantas "camadas" de perguntas ela pode fazer.
    * Se deixar `None` (padr√£o), ela cresce at√© n√£o poder mais -> **GRANDE CHANCE DE OVERFITTING!**
        * **Por qu√™?** Sem limite, a √°rvore cria regras super espec√≠ficas s√≥ pra garantir que cada folha seja 100% pura. Ela acaba **"decorando"** os dados de treino em vez de aprender o padr√£o geral.
    * √â a principal ferramenta pra **"podar"** (limitar) a √°rvore.
    * *Dica: Come√ßar com um valor baixo (3, 4, 5) √© uma boa pr√°tica.*

* `min_samples_split=2` (ou um n√∫mero maior)
    * **N√∫mero m√≠nimo de amostras** que um n√≥ precisa ter pra poder ser dividido (pra fazer uma nova pergunta).
    * Ajuda a evitar que a √°rvore crie regras pra grupos muito pequenos e espec√≠ficos. -> Combate o overfitting.
        * **Por qu√™?** Isso impede que a √°rvore crie uma nova 'regra' (uma nova pergunta) baseada em pouqu√≠ssimos exemplos. Se aumentamos para `min_samples_split=20`, a √°rvore s√≥ vai se dar ao trabalho de criar uma nova divis√£o se tiver um grupo de, no m√≠nimo, 20 amostras ("linhas"). Isso for√ßa o modelo a focar em padr√µes que aparecem em grupos maiores, que tendem a ser mais gerais e menos baseados em ru√≠do.

* `min_samples_leaf=1` (ou um n√∫mero maior)
    * **N√∫mero m√≠nimo de amostras** que um n√≥ final (uma "folha") precisa ter.
    * Garante que nossas respostas n√£o sejam baseadas em um √∫nico exemplo isolado. -> Tamb√©m combate o overfitting.
        * **Por qu√™?** Este par√¢metro olha para o *resultado* de uma divis√£o. Se uma pergunta fosse dividir um grupo de 30 amostras em dois novos grupos: um com 29 e outro com apenas 1, `min_samples_leaf=5` **proibiria** essa divis√£o. Ele for√ßa que qualquer regra criada resulte em grupos finais com um m√≠nimo de 5 amostras cada. Isso impede que a √°rvore crie regras super espec√≠ficas para isolar um √∫nico "dado teimoso", garantindo que cada previs√£o final seja baseada em um consenso de um grupo, e n√£o em um ponto fora da curva.

* `random_state=42` (ou qualquer n√∫mero)
    * **IMPORTANT√çSSIMO P/ REPRODUZIBILIDADE!**
    * Garante que o resultado seja **sempre o mesmo** toda vez que a gente rodar o c√≥digo.
    * A √°rvore usa um pouco de aleatoriedade, ent√£o sem isso, cada execu√ß√£o pode dar um resultado um tiquinho diferente. Usar um n√∫mero fixo garante que nosso experimento seja confi√°vel.
        * **Por qu√™?** De onde vem a aleatoriedade? Se a √°rvore est√° em d√∫vida entre duas perguntas (ex: "idade > 40" e "sal√°rio > 50k") e ambas geram **exatamente a mesma melhoria** de pureza, o algoritmo precisa de um crit√©rio de desempate. Ele escolhe uma delas aleatoriamente. Se n√£o usarmos o `random_state`, cada vez que rodarmos o c√≥digo, ele pode desempatar de um jeito diferente, gerando uma √°rvore levemente diferente e, consequentemente, resultados diferentes. O `random_state=42` √© como dizer: "toda vez que tiver um empate, desempate sempre do mesmo jeito".


----

# Anota√ß√µes - Support Vector Machine (SVM) ü¶æ

### Ideia Principal
* Pense em separar dois grupos de pontos em um papel.
* O SVM n√£o quer s√≥ tra√ßar uma linha qualquer pra separar. Ele quer encontrar a **"avenida" mais larga poss√≠vel** entre os dois grupos.
* Os pontos que ficam na beirada dessa avenida s√£o os mais importantes.

### O que √©?
* Modelo de ML usado principalmente para classifica√ß√£o.
* Busca encontrar um "hiperplano" (uma linha, num plano 2D, ou um plano, num espa√ßo 3D, etc.) que melhor divide os dados em classes.
* O "melhor" hiperplano √© aquele com a **margem m√°xima**.

### Como funciona? (A parte importante)

**A grande d√∫vida:** Como o SVM decide qual √© a "melhor" linha/avenida para separar os grupos?

**Resposta: MAXIMIZAR A MARGEM.**

* **Margem:** √â a dist√¢ncia entre a linha de separa√ß√£o central e os pontos mais pr√≥ximos de cada classe. Pense nela como a largura total da "avenida".
* **Vetores de Suporte (Support Vectors):** S√£o os pontos de dados que ficam exatamente na beirada da margem (no "meio-fio" da avenida). Eles s√£o os pontos mais dif√≠ceis de classificar e s√£o os √∫nicos que o modelo usa para definir a fronteira. Se a gente remover qualquer outro ponto que n√£o seja um vetor de suporte, a "avenida" n√£o muda.

**OBJETIVO DO ALGORITMO:** Encontrar a linha que cria a **avenida mais larga poss√≠vel**, pois isso torna o modelo mais robusto para classificar novos dados.

* *E se os dados n√£o puderem ser separados por uma linha reta? Aqui entra a m√°gica:*
* **O Truque do Kernel (Kernel Trick):** √â uma fun√ß√£o matem√°tica que projeta os dados para uma dimens√£o maior, onde eles magicamente se tornam separ√°veis por uma linha (ou plano).
    * **Analogia:** Imagine pontos azuis no centro de um prato e vermelhos em volta (imposs√≠vel separar com uma linha). O Kernel Trick seria como bater com for√ßa embaixo do prato, jogando os pontos azuis para o alto. Agora, em 3D, voc√™ pode passar uma "folha de papel" (um plano) horizontalmente para separar perfeitamente os pontos azuis dos vermelhos.

### Vantagens e Desvantagens

**üëç Vantagens:**
* Muito eficaz em espa√ßos de alta dimens√£o (muitas features).
* Eficiente em termos de mem√≥ria, pois usa apenas os "vetores de suporte".
* Vers√°til gra√ßas ao "Truque do Kernel".

**üëé Desvantagens:**
* **ESCALONAMENTO DE DADOS!!! (MUITO CUIDADO AQUI)**
* O que √©? O SVM √© muito sens√≠vel √† escala das features. Se uma feature (ex: sal√°rio) tem valores na casa dos milhares e outra (ex: idade) na casa das dezenas, a feature de maior escala vai dominar o modelo.
* **Solu√ß√£o:** √â **obrigat√≥rio** normalizar ou padronizar os dados (ex: com `StandardScaler` do `sklearn`).
* Pode ser lento em datasets muito grandes.

### M√£o na Massa: O `SVC` do `sklearn` üêç

Na pr√°tica, usamos a biblioteca `scikit-learn`. `SVC` significa "Support Vector Classification".

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

# 1. √â CRUCIAL escalar os dados primeiro!
scaler = StandardScaler()
X_treino_scaled = scaler.fit_transform(X_treino)
# X_teste_scaled = scaler.transform(X_teste) # Usa o mesmo scaler!

# 2. Criando o modelo com os par√¢metros padr√£o
modelo_svm = SVC()

# 3. Depois √© s√≥ treinar com os dados J√Å ESCALADOS
# modelo_svm.fit(X_treino_scaled, y_treino)
```

### Hiperpar√¢metros importantes para criar o modelo

```python
    SVC(C=1.0, kernel='rbf', gamma='scale', random_state=42)
```

* `C=1.0` **(Par√¢metro de Regulariza√ß√£o)**
    * Controla o equil√≠brio entre maximizar a margem e minimizar o erro de classifica√ß√£o.
    * **C baixo:** Prioriza uma **margem larga**, mesmo que isso signifique classificar errado alguns pontos do treino. √â uma "margem suave" que gera um modelo mais simples e com **menor chance de overfitting**.
    * **C alto:** Tenta classificar **corretamente todos** os pontos de treino, o que pode levar a uma margem mais estreita e a um modelo mais complexo. **GRANDE CHANCE DE OVERFITTING!**
    > **Analogia:** `C` √© o "pre√ßo" que o modelo paga por cada erro. Com `C` alto, o pre√ßo √© caro, ent√£o ele evita erros a todo custo, se ajustando demais aos dados de treino.

* `kernel='rbf'` **(ou `'linear'`, `'poly'`)**
    * √â aqui que a gente escolhe o "Truque do Kernel" para lidar com a complexidade dos dados.
    * `'linear'`: Para dados que voc√™ acredita serem linearmente separ√°veis. √â o mais simples e r√°pido.
    * `'rbf'` (Radial Basis Function): √â o padr√£o e um √≥timo ponto de partida. Funciona bem para a maioria dos casos complexos e n√£o-lineares, criando fronteiras baseadas em dist√¢ncia.
    * `'poly'`: Usa uma fun√ß√£o polinomial para criar fronteiras curvas.

* `gamma='scale'` **(ou um n√∫mero, ex: `0.1`)**
    * Define o alcance da influ√™ncia de um √∫nico ponto de treino. **S√≥ afeta kernels n√£o-lineares como `'rbf'` e `'poly'`.**
    * **`gamma` baixo:** A influ√™ncia de um ponto √© **grande** (longo alcance). A fronteira de decis√£o √© mais suave e geral. Pode levar a *underfitting*.
    * **`gamma` alto:** A influ√™ncia de um ponto √© **pequena** (curto alcance). A fronteira de decis√£o fica mais irregular e se ajusta muito aos dados de treino. Pode levar a *overfitting*.
    > **Por qu√™?** Um `gamma` alto significa que o modelo considera apenas os pontos muito pr√≥ximos para tomar uma decis√£o, ignorando o "quadro geral". `gamma='scale'` (padr√£o) √© uma escolha segura que se ajusta automaticamente com base nos seus dados.

* `random_state=42` **(ou qualquer n√∫mero)**
    * **IMPORTANT√çSSIMO P/ REPRODUZIBILIDADE!**
    * Embora o algoritmo do SVM seja determin√≠stico, ele usa um gerador de n√∫meros aleat√≥rios para algumas tarefas internas (como quando se usa `probability=True`).
    * Usar um `random_state` fixo garante que, sob as mesmas condi√ß√µes, o resultado ser√° **sempre o mesmo**, o que √© crucial para comparar experimentos e garantir a confiabilidade do seu trabalho.

