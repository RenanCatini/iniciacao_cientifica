# Anota√ß√µes - √Årvore de Decis√£o üå≥

**Mat√©ria:** Machine Learning
**Data:** 21/09/2025

---

### Ideia Principal
* √â tipo um jogo de "Adivinhe Quem?".
* Faz perguntas (sim/n√£o) pra ir eliminando as op√ß√µes at√© sobrar uma s√≥.

### O que √©?
* Modelo de ML em formato de fluxograma / √°rvore.
* Toma decis√µes com base numa sequ√™ncia de perguntas sobre os dados.
* Cada pergunta quebra os dados em grupos menores.

### Como funciona? (A parte importante)

**A grande d√∫vida:** Como a √°rvore escolhe a "melhor" pergunta pra fazer em cada etapa?

**Resposta: PUREZA de um grupo.**

* **Grupo Puro:** Todo mundo no grupo tem a mesma classifica√ß√£o/resposta.
    * Ex: Caixa s√≥ com ma√ß√£s.
    * √â o nosso objetivo. Chegou num grupo puro = FIM. Vira uma **"folha"** da √°rvore.

* **Grupo Impuro (Misto):** As respostas est√£o misturadas.
    * Ex: Caixa com ma√ß√£s e laranjas.
    * PROBLEMA! Precisa fazer mais perguntas pra tentar separar.

**OBJETIVO DO ALGORITMO:** Fazer a pergunta que **mais aumenta a pureza** (ou mais diminui a impureza) dos grupos que v√£o ser criados.

* *Lembrete: Como ele mede a impureza? Usa umas f√≥rmulas matem√°ticas tipo **√çndice Gini** ou **Entropia**, que basicamente d√£o notas para o grupo, o qu√£o impuro ou puro ele est√°.*

### Vantagens e Desvantagens

**üëç Vantagens:**
* F√°cil de entender e visualizar. D√° pra desenhar.
* N√£o precisa de mto pr√©-processamento de dados.

**üëé Desvantagens:**
* **OVERFITTING!!! (MUITO CUIDADO AQUI)**
* O que √©? A √°rvore fica complexa demais e "decora" os dados de treino.
* Ela n√£o aprende a regra geral, s√≥ os exemplos espec√≠ficos.
* **Analogia:** O aluno que decora as respostas da prova. Se mudar a pergunta, ele erra.

### M√£o na Massa: O `DecisionTreeClassifier` do `sklearn` üêç

Na pr√°tica, a gente n√£o implementa isso do zero. Usamos a biblioteca `scikit-learn`.

```python
from sklearn.tree import DecisionTreeClassifier

# Criando o modelo com os par√¢metros padr√£o
modelo_arvore = DecisionTreeClassifier()

# Depois √© s√≥ treinar com os dados
# modelo_arvore.fit(X_treino, y_treino)
```

**Hiperpar√¢metros importantes na hora de criar o modelo:**

```python
    DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, random_state=42)
```


* `criterion='gini'` (ou `'entropy')`
    * √â aqui que a gente escolhe a "r√©gua" pra medir a impureza. Lembra das 'notas'? √â isso.
    * `'gini'` √© o padr√£o e geralmente **mais r√°pido**, funciona calculando a probabilidade de errar (se tiver uma caixa com apenas 1 tipo, o gini=0, se tiver 2 tipos 50/50, o gini ser√° de 0.5). Vai de 0 at√© 0.5 (para 2 classes).
        * Calcula a partir da multiplica√ß√£o simples de probabilidades.
        * $$I_G(p) = 1 - \sum_{i=1}^{k} (p_i)^2$$
        * Onde:
            * **$k$**: √â o n√∫mero total de classes.
            * **$p_i$**: √â a propor√ß√£o (probabilidade) da classe *i* no grupo.
    * `'entropy'` √© geralmente um pouco **mais lento**, funciona calculando o n√≠vel de 'surpresa' ou 'desordem' (se tiver uma caixa com apenas 1 tipo, a surpresa √© 0, se tiver 2 tipos 50/50, a surpresa/incerteza √© m√°xima = 1). Vai de 0 at√© 1.0 (para 2 classes).
        * Calcula usando logaritmos, que vem da Teoria da Informa√ß√£o.
        * $$H(p) = - \sum_{i=1}^{k} p_i \log_2(p_i)$$
        * Onde:
            * **$k$**: √â o n√∫mero total de classes.
            * **$p_i$**: √â a propor√ß√£o (probabilidade) da classe *i* no grupo.

* `max_depth=None` (ou um n√∫mero, ex: `3`)
    * **Profundidade m√°xima** da √°rvore. Quantas "camadas" de perguntas ela pode fazer.
    * Se deixar `None` (padr√£o), ela cresce at√© n√£o poder mais -> **GRANDE CHANCE DE OVERFITTING!**
        * **Por qu√™?** Sem limite, a √°rvore cria regras super espec√≠ficas s√≥ pra garantir que cada folha seja 100% pura. Ela acaba **"decorando"** os dados de treino em vez de aprender o padr√£o geral.
    * √â a principal ferramenta pra **"podar"** (limitar) a √°rvore.
    * *Dica: Come√ßar com um valor baixo (3, 4, 5) √© uma boa pr√°tica.*

* `min_samples_split=2` (ou um n√∫mero maior)
    * **N√∫mero m√≠nimo de amostras** que um n√≥ precisa ter pra poder ser dividido (pra fazer uma nova pergunta).
    * Ajuda a evitar que a √°rvore crie regras pra grupos muito pequenos e espec√≠ficos. -> Combate o overfitting.
        * **Por qu√™?** Isso impede que a √°rvore crie uma nova 'regra' (uma nova pergunta) baseada em pouqu√≠ssimos exemplos. Se aumentamos para `min_samples_split=20`, a √°rvore s√≥ vai se dar ao trabalho de criar uma nova divis√£o se tiver um grupo de, no m√≠nimo, 20 amostras ("linhas"). Isso for√ßa o modelo a focar em padr√µes que aparecem em grupos maiores, que tendem a ser mais gerais e menos baseados em ru√≠do.

* `min_samples_leaf=1` (ou um n√∫mero maior)
    * **N√∫mero m√≠nimo de amostras** que um n√≥ final (uma "folha") precisa ter.
    * Garante que nossas respostas n√£o sejam baseadas em um √∫nico exemplo isolado. -> Tamb√©m combate o overfitting.
        * **Por qu√™?** Este par√¢metro olha para o *resultado* de uma divis√£o. Se uma pergunta fosse dividir um grupo de 30 amostras em dois novos grupos: um com 29 e outro com apenas 1, `min_samples_leaf=5` **proibiria** essa divis√£o. Ele for√ßa que qualquer regra criada resulte em grupos finais com um m√≠nimo de 5 amostras cada. Isso impede que a √°rvore crie regras super espec√≠ficas para isolar um √∫nico "dado teimoso", garantindo que cada previs√£o final seja baseada em um consenso de um grupo, e n√£o em um ponto fora da curva.

* `random_state=42` (ou qualquer n√∫mero)
    * **IMPORTANT√çSSIMO P/ REPRODUZIBILIDADE!**
    * Garante que o resultado seja **sempre o mesmo** toda vez que a gente rodar o c√≥digo.
    * A √°rvore usa um pouco de aleatoriedade, ent√£o sem isso, cada execu√ß√£o pode dar um resultado um tiquinho diferente. Usar um n√∫mero fixo garante que nosso experimento seja confi√°vel.
        * **Por qu√™?** De onde vem a aleatoriedade? Se a √°rvore est√° em d√∫vida entre duas perguntas (ex: "idade > 40" e "sal√°rio > 50k") e ambas geram **exatamente a mesma melhoria** de pureza, o algoritmo precisa de um crit√©rio de desempate. Ele escolhe uma delas aleatoriamente. Se n√£o usarmos o `random_state`, cada vez que rodarmos o c√≥digo, ele pode desempatar de um jeito diferente, gerando uma √°rvore levemente diferente e, consequentemente, resultados diferentes. O `random_state=42` √© como dizer: "toda vez que tiver um empate, desempate sempre do mesmo jeito".

